{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c43c3662",
   "metadata": {},
   "source": [
    "# üß™ Great Expectations avec Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86190845",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß D√©marrage de Spark avec Iceberg...\")\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import boto3\n",
    "\n",
    "# üîê Configuration MinIO\n",
    "MINIO_ENDPOINT = os.getenv(\"S3_ENDPOINT_URL\", \"http://minio:9000\")\n",
    "AWS_ACCESS_KEY = os.getenv(\"AWS_ACCESS_KEY_ID\", \"minio\")\n",
    "AWS_SECRET_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\", \"minio123\")\n",
    "BUCKET_RAW = \"retail-raw\"\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"RetailGX\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Spark + Iceberg pr√™t.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd11849",
   "metadata": {},
   "source": [
    "# üìú Cr√©er suite de validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3ac2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Validation Great Expectations\n",
    "print(\"üîç Configuration Great Expectations (Spark engine)...\")\n",
    "\n",
    "import great_expectations as gx\n",
    "from great_expectations.core.expectation_suite import ExpectationSuite\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "context = gx.get_context()\n",
    "datasource = context.data_sources.add_spark(name=\"iceberg\")\n",
    "\n",
    "# --- 1. Stores ---\n",
    "stores_suite = ExpectationSuite(name=\"stores_suite\")\n",
    "stores_suite.add_expectation(gx.expectations.ExpectColumnToExist(column=\"store_id\"))\n",
    "stores_suite.add_expectation(gx.expectations.ExpectColumnValuesToNotBeNull(column=\"store_id\"))\n",
    "stores_suite.add_expectation(gx.expectations.ExpectColumnValuesToBeUnique(column=\"store_id\"))\n",
    "stores_suite.add_expectation(gx.expectations.ExpectColumnToExist(column=\"country\"))\n",
    "stores_suite.add_expectation(gx.expectations.ExpectColumnValuesToBeInSet(\n",
    "    column=\"country\", \n",
    "    value_set=[\"France\"]  # ajuste si besoin\n",
    "))\n",
    "stores_suite.add_expectation(gx.expectations.ExpectColumnToExist(column=\"opening_date\"))\n",
    "stores_suite.add_expectation(gx.expectations.ExpectColumnValuesToNotBeNull(column=\"opening_date\"))\n",
    "context.suites.add(stores_suite)\n",
    "\n",
    "# --- 2. Products ---\n",
    "products_suite = ExpectationSuite(name=\"products_suite\")\n",
    "products_suite.add_expectation(gx.expectations.ExpectColumnToExist(column=\"product_id\"))\n",
    "products_suite.add_expectation(gx.expectations.ExpectColumnValuesToNotBeNull(column=\"product_id\"))\n",
    "products_suite.add_expectation(gx.expectations.ExpectColumnValuesToBeUnique(column=\"product_id\"))\n",
    "products_suite.add_expectation(gx.expectations.ExpectColumnToExist(column=\"list_price\"))\n",
    "products_suite.add_expectation(gx.expectations.ExpectColumnValuesToBeBetween(\n",
    "    column=\"list_price\", \n",
    "    min_value=0.01, \n",
    "    max_value=100000.0\n",
    "))\n",
    "context.suites.add(products_suite)\n",
    "\n",
    "# --- 3. Sales ---\n",
    "sales_suite = ExpectationSuite(name=\"sales_suite\")\n",
    "sales_suite.add_expectation(gx.expectations.ExpectColumnToExist(column=\"sale_id\"))\n",
    "sales_suite.add_expectation(gx.expectations.ExpectColumnValuesToNotBeNull(column=\"sale_id\"))\n",
    "sales_suite.add_expectation(gx.expectations.ExpectColumnValuesToBeUnique(column=\"sale_id\"))\n",
    "sales_suite.add_expectation(gx.expectations.ExpectColumnToExist(column=\"quantity\"))\n",
    "sales_suite.add_expectation(gx.expectations.ExpectColumnValuesToBeBetween(\n",
    "    column=\"quantity\", \n",
    "    min_value=1, \n",
    "    max_value=10000\n",
    "))\n",
    "sales_suite.add_expectation(gx.expectations.ExpectColumnToExist(column=\"sale_date\"))\n",
    "sales_suite.add_expectation(gx.expectations.ExpectColumnValuesToNotBeNull(column=\"sale_date\"))\n",
    "context.suites.add(sales_suite)\n",
    "\n",
    "# --- 4. Employees ---\n",
    "employees_suite = ExpectationSuite(name=\"employees_suite\")\n",
    "employees_suite.add_expectation(gx.expectations.ExpectColumnToExist(column=\"employee_id\"))\n",
    "employees_suite.add_expectation(gx.expectations.ExpectColumnValuesToNotBeNull(column=\"employee_id\"))\n",
    "employees_suite.add_expectation(gx.expectations.ExpectColumnValuesToBeUnique(column=\"employee_id\"))\n",
    "employees_suite.add_expectation(gx.expectations.ExpectColumnToExist(column=\"store_id\"))\n",
    "employees_suite.add_expectation(gx.expectations.ExpectColumnValuesToNotBeNull(column=\"store_id\"))\n",
    "employees_suite.add_expectation(gx.expectations.ExpectColumnToExist(column=\"job_title\"))\n",
    "employees_suite.add_expectation(gx.expectations.ExpectColumnValuesToNotBeNull(column=\"job_title\"))\n",
    "context.suites.add(employees_suite)\n",
    "\n",
    "suites = {\n",
    "    \"stores\": stores_suite,\n",
    "    \"products\": products_suite,\n",
    "    \"sales\": sales_suite,\n",
    "    \"employees\": employees_suite,\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Suites cr√©√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d85823",
   "metadata": {},
   "source": [
    "# Boucle pour g√©n√©rer la validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bee4031",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 2. Validation ---\n",
    "tables = {\n",
    "    \"stores\": \"retail.raw.stores\",\n",
    "    \"products\": \"retail.raw.products\",\n",
    "    \"sales\": \"retail.raw.sales\",\n",
    "    \"employees\": \"retail.raw.employees\",\n",
    "}\n",
    "\n",
    "validation_results = []\n",
    "all_success = True\n",
    "for name, table_ref in tables.items():\n",
    "    print(f\"\\n‚û°Ô∏è  Validation de '{name}'...\")\n",
    "    try:\n",
    "        df = spark.sql(f\"SELECT * FROM {table_ref}\")\n",
    "        print(f\"   üìä {df.count()} lignes.\")\n",
    "        \n",
    "        # ‚úÖ Cr√©ation simple\n",
    "        asset = datasource.add_dataframe_asset(name=name)\n",
    "        batch_def = asset.add_batch_definition_whole_dataframe(\"default\")\n",
    "        batch = batch_def.get_batch(batch_parameters={\"dataframe\": df})\n",
    "        \n",
    "        # ‚úÖ Validator\n",
    "        validator = context.get_validator(\n",
    "            batch=batch, \n",
    "            expectation_suite=suites[name]\n",
    "        )\n",
    "        \n",
    "        result = validator.validate()\n",
    "        \n",
    "        validation_results.append({\"table\": name, \"result\": result, \"success\": result.success})\n",
    "        status = \"‚úÖ OK\" if result.success else \"‚ùå √âchec\"\n",
    "        print(f\"   {status} ({len(result.results)} expectations)\")\n",
    "        all_success &= result.success\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Erreur : {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()  # utile pour debug pr√©cis\n",
    "        validation_results.append({\"table\": name, \"error\": str(e), \"success\": False})\n",
    "        all_success = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8631cc6e",
   "metadata": {},
   "source": [
    "# --- 3. Rapport simple (sans gx.util) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_path = \"/tmp/gx_validation_simple.txt\"\n",
    "with open(report_path, \"w\") as f:\n",
    "    f.write(f\"Validation Iceberg ‚Äî {datetime.now()}\\n\")\n",
    "    f.write(\"=\"*50 + \"\\n\")\n",
    "    for vr in validation_results:\n",
    "        status = \"OK\" if vr[\"success\"] else (\"ERROR\" if \"error\" in vr else \"FAIL\")\n",
    "        f.write(f\"{vr['table']:<12} : {status}\\n\")\n",
    "        if not vr[\"success\"]:\n",
    "            f.write(f\"  ‚Üí {vr.get('error', 'see HTML')}\\n\")\n",
    "    f.write(\"=\"*50 + \"\\n\")\n",
    "    f.write(f\"R√©sultat global : {'SUCCESS' if all_success else 'FAILURE'}\\n\")\n",
    "\n",
    "print(f\"\\n‚úÖ Rapport texte sauvegard√© : {report_path}\")\n",
    "\n",
    "# R√©sum√©\n",
    "print(\"\\nüìã R√âSUM√â\")\n",
    "for vr in validation_results:\n",
    "    print(f\" ‚Ä¢ {vr['table']:<12} ‚Üí {'‚úÖ' if vr['success'] else '‚ùå'}\")\n",
    "print(f\"\\n{'üéâ TOUT OK' if all_success else '‚ö†Ô∏è Probl√®mes d√©tect√©s'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8e2122",
   "metadata": {},
   "source": [
    "# üì§ Export m√©triques qualit√© vers Iceberg (pour Superset) Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0946ca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßæ Sauvegarde des r√©sultats de validation dans Iceberg\n",
    "print(\"\\nüíæ Sauvegarde des r√©sultats de validation dans Iceberg...\")\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "import uuid\n",
    "\n",
    "# --- 1. Pr√©parer les donn√©es ---\n",
    "results_data = []\n",
    "run_id = str(uuid.uuid4())\n",
    "run_at = datetime.now(timezone.utc)\n",
    "\n",
    "for vr in validation_results:\n",
    "    table_name = vr[\"table\"]\n",
    "    success = vr[\"success\"]\n",
    "    \n",
    "    if \"error\" in vr:\n",
    "        total = 0\n",
    "        failed = 0\n",
    "        error_msg = vr[\"error\"]\n",
    "    else:\n",
    "        res = vr[\"result\"]\n",
    "        total = len(res.results)\n",
    "        failed = sum(1 for r in res.results if not r.success)\n",
    "        error_msg = None\n",
    "    \n",
    "    results_data.append((\n",
    "        run_id,\n",
    "        table_name,\n",
    "        success,\n",
    "        run_at,\n",
    "        total,\n",
    "        failed,\n",
    "        error_msg\n",
    "    ))\n",
    "\n",
    "# --- 2. Cr√©er DataFrame Spark ---\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, BooleanType, \n",
    "    IntegerType, TimestampType\n",
    ")\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"run_id\", StringType(), False),\n",
    "    StructField(\"table_name\", StringType(), False),\n",
    "    StructField(\"success\", BooleanType(), False),\n",
    "    StructField(\"run_at\", TimestampType(), False),\n",
    "    StructField(\"total_expectations\", IntegerType(), False),\n",
    "    StructField(\"failed_expectations\", IntegerType(), False),\n",
    "    StructField(\"error_message\", StringType(), True),\n",
    "])\n",
    "\n",
    "results_df = spark.createDataFrame(results_data, schema)\n",
    "\n",
    "# --- 3. Cr√©er le namespace si besoin ---\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS retail.quality\")\n",
    "\n",
    "# --- 4. √âcrire dans Iceberg ---\n",
    "try:\n",
    "    # Si la table n'existe pas ‚Üí CREATE\n",
    "    results_df.writeTo(\"retail.quality.gx_validation_results\") \\\n",
    "              .using(\"iceberg\") \\\n",
    "              .createOrReplace()\n",
    "    print(\"‚úÖ Table 'retail.quality.gx_validation_results' cr√©√©e et remplie.\")\n",
    "except Exception as e:\n",
    "    # Si d√©j√† existante ‚Üí APPEND\n",
    "    try:\n",
    "        results_df.writeTo(\"retail.quality.gx_validation_results\") \\\n",
    "                  .using(\"iceberg\") \\\n",
    "                  .append()\n",
    "        print(\"‚úÖ R√©sultats ajout√©s √† 'retail.quality.gx_validation_results'.\")\n",
    "    except Exception as append_err:\n",
    "        print(f\"‚ùå √âchec √©criture Iceberg : {append_err}\")\n",
    "        raise\n",
    "\n",
    "# --- 5. V√©rification rapide ---\n",
    "print(\"\\nüëÄ Aper√ßu des r√©sultats sauvegard√©s :\")\n",
    "spark.table(\"retail.quality.gx_validation_results\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25ac08c",
   "metadata": {},
   "source": [
    "# üßæ Sauvegarde D√âTAILL√âE des r√©sultats par expectation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b890419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßæ Sauvegarde D√âTAILL√âE des r√©sultats par expectation\n",
    "print(\"\\nüíæ Sauvegarde d√©taill√©e des r√©sultats (par expectation) dans Iceberg...\")\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "import uuid\n",
    "import json\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, BooleanType,\n",
    "    IntegerType, DoubleType, TimestampType\n",
    ")\n",
    "\n",
    "# --- 1. Collecter tous les r√©sultats d√©taill√©s ---\n",
    "detailed_results = []\n",
    "run_id = str(uuid.uuid4())\n",
    "run_at = datetime.now(timezone.utc)\n",
    "\n",
    "for vr in validation_results:\n",
    "    table_name = vr[\"table\"]\n",
    "    \n",
    "    if \"error\" in vr:\n",
    "        detailed_results.append({\n",
    "            \"run_id\": run_id,\n",
    "            \"table_name\": table_name,\n",
    "            \"expectation_id\": \"global_error\",\n",
    "            \"expectation_type\": \"global_error\",\n",
    "            \"column_name\": None,\n",
    "            \"kwargs\": json.dumps({}),\n",
    "            \"success\": False,\n",
    "            \"observed_value\": None,\n",
    "            \"unexpected_count\": 0,\n",
    "            \"unexpected_percent\": 0.0,\n",
    "            \"partial_unexpected_list\": json.dumps([]),\n",
    "            \"error_message\": vr[\"error\"][:1000],  # tronquer si tr√®s long\n",
    "            \"run_at\": run_at\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    validation_result = vr[\"result\"]\n",
    "    for expectation_result in validation_result.results:\n",
    "        config = expectation_result.expectation_config\n",
    "        result = expectation_result.result or {}\n",
    "        \n",
    "        # üîë R√©cup√©ration robuste du type\n",
    "        exp_type = (\n",
    "            getattr(config, \"type\", None)\n",
    "            or getattr(config, \"expectation_type\", None)\n",
    "            or str(type(config)).split(\".\")[-1].rstrip(\"'>\")\n",
    "        )\n",
    "        \n",
    "        # üîë R√©cup√©ration robuste des kwargs\n",
    "        kwargs = getattr(config, \"kwargs\", None) or {}\n",
    "        if not isinstance(kwargs, dict):\n",
    "            kwargs = {}\n",
    "        \n",
    "        column = kwargs.get(\"column\")  # toujours pr√©sent dans les expectations classiques\n",
    "        \n",
    "        # Nettoyer kwargs pour JSON\n",
    "        clean_kwargs = {}\n",
    "        for k, v in kwargs.items():\n",
    "            if isinstance(v, (str, int, float, bool, type(None))):\n",
    "                clean_kwargs[k] = v\n",
    "            elif isinstance(v, (list, tuple)):\n",
    "                clean_kwargs[k] = [x for x in v if isinstance(x, (str, int, float, bool))]\n",
    "            # skip les objets complexes (fonctions, DataFrames, etc.)\n",
    "        \n",
    "        # Valeurs observ√©es\n",
    "        observed_value = result.get(\"observed_value\")\n",
    "        if observed_value is not None and not isinstance(observed_value, (str, int, float, bool)):\n",
    "            try:\n",
    "                observed_value = json.dumps(observed_value, default=str)\n",
    "            except:\n",
    "                observed_value = str(observed_value)[:200]\n",
    "        \n",
    "        # Liste partielle des inattendus\n",
    "        partial_unexpected = result.get(\"partial_unexpected_list\", [])\n",
    "        try:\n",
    "            partial_unexpected_str = json.dumps(\n",
    "                [str(x)[:100] for x in partial_unexpected[:10]], \n",
    "                default=str\n",
    "            )\n",
    "        except Exception:\n",
    "            partial_unexpected_str = \"[]\"\n",
    "        \n",
    "        detailed_results.append({\n",
    "            \"run_id\": run_id,\n",
    "            \"table_name\": table_name,\n",
    "            \"expectation_id\": f\"{exp_type}_{column or 'global'}_{abs(hash(json.dumps(clean_kwargs, sort_keys=True))) % 10**6}\",\n",
    "            \"expectation_type\": str(exp_type),\n",
    "            \"column_name\": str(column) if column is not None else None,\n",
    "            \"kwargs\": json.dumps(clean_kwargs, default=str),\n",
    "            \"success\": bool(expectation_result.success),\n",
    "            \"observed_value\": str(observed_value) if observed_value is not None else None,\n",
    "            \"unexpected_count\": int(result.get(\"unexpected_count\", 0)),\n",
    "            \"unexpected_percent\": float(result.get(\"unexpected_percent\", 0.0)),\n",
    "            \"partial_unexpected_list\": partial_unexpected_str,\n",
    "            \"error_message\": None,\n",
    "            \"run_at\": run_at\n",
    "        })\n",
    "\n",
    "# --- 2. Cr√©er DataFrame ---\n",
    "schema = StructType([\n",
    "    StructField(\"run_id\", StringType(), False),\n",
    "    StructField(\"table_name\", StringType(), False),\n",
    "    StructField(\"expectation_id\", StringType(), False),\n",
    "    StructField(\"expectation_type\", StringType(), False),\n",
    "    StructField(\"column_name\", StringType(), True),\n",
    "    StructField(\"kwargs\", StringType(), True),\n",
    "    StructField(\"success\", BooleanType(), False),\n",
    "    StructField(\"observed_value\", StringType(), True),\n",
    "    StructField(\"unexpected_count\", IntegerType(), True),\n",
    "    StructField(\"unexpected_percent\", DoubleType(), True),\n",
    "    StructField(\"partial_unexpected_list\", StringType(), True),\n",
    "    StructField(\"error_message\", StringType(), True),\n",
    "    StructField(\"run_at\", TimestampType(), False),\n",
    "])\n",
    "\n",
    "detailed_df = spark.createDataFrame(detailed_results, schema)\n",
    "\n",
    "# --- 3. Cr√©er namespace + table ---\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS retail.quality\")\n",
    "\n",
    "# Cr√©er ou ins√©rer\n",
    "try:\n",
    "    detailed_df.writeTo(\"retail.quality.gx_expectation_results\") \\\n",
    "               .using(\"iceberg\") \\\n",
    "               .tableProperty(\"write.format.default\", \"parquet\") \\\n",
    "               .tableProperty(\"history.expire.max-snapshot-age-ms\", \"604800000\") \\\n",
    "               .createOrReplace()\n",
    "    print(\"‚úÖ Table 'retail.quality.gx_expectation_results' cr√©√©e.\")\n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e).lower():\n",
    "        detailed_df.writeTo(\"retail.quality.gx_expectation_results\") \\\n",
    "                   .using(\"iceberg\") \\\n",
    "                   .append()\n",
    "        print(\"‚úÖ R√©sultats ajout√©s √† 'retail.quality.gx_expectation_results'.\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# --- 4. V√©rification ---\n",
    "print(\"\\nüîç Exemple de r√©sultats d√©taill√©s (5 premi√®res expectations) :\")\n",
    "spark.sql(\"SELECT * FROM retail.quality.gx_expectation_results\") \\\n",
    "     .select(\"*\") \\\n",
    "     .show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef956df",
   "metadata": {},
   "source": [
    "# üì¶ Sauvegarde JSON compl√®te et robuste de l'ex√©cution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28258d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì§ Sauvegarde JSON compl√®te dans S3 via boto3 (1 seul fichier .json)\n",
    "print(\"\\nüì§ Sauvegarde JSON dans S3 avec boto3...\")\n",
    "\n",
    "import json\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# --- 1. G√©n√©rer le rapport d√©taill√© (robuste) ---\n",
    "run_summary = {\n",
    "    \"run_id\": run_id,\n",
    "    \"run_at\": run_at.isoformat(),\n",
    "    \"spark_version\": spark.version,\n",
    "    \"gx_version\": getattr(gx, '__version__', 'unknown'),\n",
    "    \"tables\": []\n",
    "}\n",
    "\n",
    "for vr in validation_results:\n",
    "    table_entry = {\n",
    "        \"name\": vr[\"table\"],\n",
    "        \"success\": vr[\"success\"],\n",
    "        \"expectations\": []\n",
    "    }\n",
    "    \n",
    "    if \"error\" in vr:\n",
    "        table_entry[\"error\"] = str(vr[\"error\"])[:2000]\n",
    "    elif vr.get(\"result\"):\n",
    "        for er in (vr[\"result\"].results or []):\n",
    "            config = er.expectation_config\n",
    "            exp_type = (\n",
    "                getattr(config, \"type\", None)\n",
    "                or getattr(config, \"expectation_type\", None)\n",
    "                or \"unknown\"\n",
    "            )\n",
    "            kwargs = getattr(config, \"kwargs\", {}) or {}\n",
    "            column = kwargs.get(\"column\")\n",
    "            result = er.result or {}\n",
    "            \n",
    "            # Valeurs safe pour JSON\n",
    "            unexpected_count = result.get(\"unexpected_count\", 0)\n",
    "            observed_value = result.get(\"observed_value\")\n",
    "            if observed_value is not None and not isinstance(observed_value, (str, int, float, bool, type(None))):\n",
    "                observed_value = str(observed_value)[:500]\n",
    "            \n",
    "            table_entry[\"expectations\"].append({\n",
    "                \"type\": str(exp_type),\n",
    "                \"column\": str(column) if column is not None else None,\n",
    "                \"success\": bool(er.success),\n",
    "                \"unexpected_count\": int(unexpected_count),\n",
    "                \"observed_value\": observed_value,\n",
    "                \"kwargs_summary\": {\n",
    "                    k: v for k, v in kwargs.items()\n",
    "                    if k in [\"column\", \"min_value\", \"max_value\", \"value_set\", \"regex\", \"mostly\"]\n",
    "                    and isinstance(v, (str, int, float, bool, list))\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    run_summary[\"tables\"].append(table_entry)\n",
    "\n",
    "# --- 2. S√©rialiser en JSON ---\n",
    "try:\n",
    "    json_bytes = json.dumps(run_summary, indent=2, ensure_ascii=False, default=str).encode('utf-8')\n",
    "    print(f\"‚úÖ JSON g√©n√©r√© ({len(json_bytes)} octets)\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erreur s√©rialisation JSON : {e}\")\n",
    "    json_bytes = json.dumps({\n",
    "        \"run_id\": run_id,\n",
    "        \"run_at\": run_at.isoformat(),\n",
    "        \"error\": f\"JSON serialization failed: {str(e)}\",\n",
    "        \"summary\": [{\"table\": vr[\"table\"], \"success\": vr[\"success\"]} for vr in validation_results]\n",
    "    }, default=str).encode('utf-8')\n",
    "\n",
    "# --- 3. √âcrire dans S3 avec boto3 ---\n",
    "s3_key = f\"gx_runs/{run_id}.json\"\n",
    "MINIO_ENDPOINT = os.getenv(\"S3_ENDPOINT_URL\", \"http://minio:9000\")\n",
    "AWS_ACCESS_KEY = os.getenv(\"AWS_ACCESS_KEY_ID\", \"minio\")\n",
    "AWS_SECRET_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\", \"minio123\")\n",
    "BUCKET_RAW = \"retail-raw\"\n",
    "\n",
    "s3_client = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=MINIO_ENDPOINT,\n",
    "    aws_access_key_id=AWS_ACCESS_KEY,\n",
    "    aws_secret_access_key=AWS_SECRET_KEY,\n",
    ")\n",
    "try:\n",
    "    \n",
    "    s3_client.put_object(\n",
    "        Bucket=BUCKET_RAW,\n",
    "        Key=s3_key,\n",
    "        Body=json_bytes,\n",
    "        ContentType='application/json',\n",
    "        Metadata={\n",
    "            'run_id': run_id,\n",
    "            'source': 'great_expectations',\n",
    "            'table_count': str(len(validation_results))\n",
    "        }\n",
    "    )\n",
    "    s3_uri = f\"s3a://{BUCKET_RAW}/{s3_key}\"\n",
    "    print(f\"‚úÖ JSON sauvegard√© dans S3 : {s3_uri}\")\n",
    "    \n",
    "    # Optionnel : v√©rifier l'existence\n",
    "    head = s3_client.head_object(Bucket=BUCKET_RAW, Key=s3_key)\n",
    "    print(f\"   ‚ÑπÔ∏è Taille : {head['ContentLength']} octets | ETag : {head['ETag'].strip('\\\"')}\")\n",
    "\n",
    "except ClientError as e:\n",
    "    error_code = e.response['Error']['Code']\n",
    "    print(f\"‚ùå Erreur S3 ({error_code}) : {e.response['Error']['Message']}\")\n",
    "    # Fallback : sauvegarde locale si besoin\n",
    "    fallback_path = f\"/tmp/{run_id}.json\"\n",
    "    try:\n",
    "        with open(fallback_path, 'wb') as f:\n",
    "            f.write(json_bytes)\n",
    "        print(f\"‚ö†Ô∏è Fallback local : {fallback_path}\")\n",
    "    except Exception as local_err:\n",
    "        print(f\"‚ùå √âchec fallback local : {local_err}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur inattendue S3 : {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee11be4c",
   "metadata": {},
   "source": [
    "# üéâ Fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e4b836",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"\\nüéâ Pipeline termin√© !\")\n",
    "print(f\"‚û°Ô∏è  Donn√©es brutes : s3://{BUCKET_RAW}/\")\n",
    "print(f\"‚û°Ô∏è  Tables Iceberg : retail.raw.*\")\n",
    "print(f\"‚û°Ô∏è  M√©triques qualit√© : retail.raw.data_quality_metrics\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
